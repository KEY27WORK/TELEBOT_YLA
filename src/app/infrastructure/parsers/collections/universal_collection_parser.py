# üìö app/infrastructure/parsers/collections/universal_collection_parser.py
"""
üìö `UniversalCollectionParser` ‚Äî INFRA-—Ä—ñ–≤–µ–Ω—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –∫–æ–ª–µ–∫—Ü—ñ–π YoungLA.

üîπ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —î–¥–∏–Ω–∏–π HTML-–ø–∞—Ä—Å–µ—Ä (–∫–æ–Ω—Ñ—ñ–≥—É—Ä—É—î—Ç—å—Å—è —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É).  
üîπ –°–ø–µ—Ä—à—É –ø—Ä–æ–±—É—î JSON-LD, –¥–∞–ª—ñ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –¥–æ DOM + –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó.  
üîπ –ù–æ—Ä–º–∞–ª—ñ–∑—É—î URL (–≤–∏–¥–∞–ª—è—î query/fragment, –¥–æ–±—É–¥–æ–≤—É—î –∞–±—Å–æ–ª—é—Ç–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è).  
üîπ –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ —Ç–æ–≤–∞—Ä–∏.
"""

from __future__ import annotations

# üåê –ó–æ–≤–Ω—ñ—à–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏
from bs4 import BeautifulSoup												# ü•£ –†–æ–∑–±—ñ—Ä HTML

# üî† –°–∏—Å—Ç–µ–º–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏
import json																# üßæ –†–æ–±–æ—Ç–∞ –∑ JSON-LD
import logging															# üßæ –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–¥—ñ–π
import re																# üßÆ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞/–º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—è —Ä—è–¥–∫—ñ–≤
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple		# üß∞ –£–∑–≥–æ–¥–∂–µ–Ω–∞ —Ç–∏–ø—ñ–∑–∞—Ü—ñ—è

# üß© –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –º–æ–¥—É–ª—ñ –ø—Ä–æ—î–∫—Ç—É
from app.config.config_service import ConfigService					# ‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è INFRA
from app.infrastructure.web.webdriver_service import WebDriverService	# üåê –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫
from app.shared.utils.url_parser_service import UrlParserService		# üåç –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è URL

logger = logging.getLogger(__name__)									# üßæ –ú–æ–¥—É–ª—å–Ω–∏–π –ª–æ–≥–µ—Ä


# ================================
# üîß –£–¢–ò–õ–Ü–¢–ò (–°–¢–†–û–ì–û-–¢–ò–ü–û–ë–ï–ó–ü–ï–ß–ù–Ü)
# ================================
def _safe_json_loads(raw: Optional[str]) -> Optional[Any]:
    """üìÑ –ë–µ–∑–ø–µ—á–Ω–∏–π json.loads —ñ–∑ –∑–∞—Ö–∏—Å—Ç–æ–º –≤—ñ–¥ –≤–∏–Ω—è—Ç–∫—ñ–≤."""

    if not raw:
        return None														# üí§ –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None
    try:
        return json.loads(raw)											# ‚úÖ –°–ø—Ä–æ–±–∞ —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON
    except Exception:
        return None														# üö´ –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π JSON


def _uniq_keep_order(seq: Iterable[str]) -> List[str]:
    """üîÅ –ü–æ–≤–µ—Ä—Ç–∞—î —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –ø–æ—Ä—è–¥–æ–∫."""

    out: List[str] = []													# üì¶ –†–µ–∑—É–ª—å—Ç–∞—Ç
    seen: Set[str] = set()												# üëÅÔ∏è‚Äçüó®Ô∏è –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –∑—É—Å—Ç—Ä—ñ–Ω—É—Ç—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    for value in seq:
        if not value or value in seen:
            continue													# ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø—É—Å—Ç—ñ/–¥—É–±–ª—ñ
        seen.add(value)													# üîñ –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–µ
        out.append(value)												# ‚ûï –î–æ–¥–∞—î–º–æ –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    return out


def _strip_query_and_fragment(url: str) -> str:
    """‚úÇÔ∏è –í–∏–¥–∞–ª—è—î query/fragment –∑ URL."""

    if not url:
        return ""
    base = url.split("#", 1)[0]											# üßµ –ü—Ä–∏–±–∏—Ä–∞—î–º–æ fragment
    base = base.split("?", 1)[0]											# üßµ –ü—Ä–∏–±–∏—Ä–∞—î–º–æ query
    return base


def _is_product_like_path(href: str) -> bool:
    """üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –º—ñ—Å—Ç–∏—Ç—å —à–ª—è—Ö `/products/`."""

    return "/products/" in (href or "").lower()


def _ensure_abs(base_url: str, href: str) -> str:
    """üåê –ë—É–¥—É—î –∞–±—Å–æ–ª—é—Ç–Ω–∏–π URL –∑–∞ –±–∞–∑–æ–≤–∏–º –¥–æ–º–µ–Ω–æ–º."""

    if not href:
        return ""
    cleaned = href.strip()
    if not cleaned:
        return ""
    if cleaned.startswith("//"):
        return "https:" + cleaned										# üåê –ü—Ä–æ—Ç–æ–∫–æ–ª-–∑–∞–ª–µ–∂–Ω–∏–π URL
    if cleaned.startswith("/"):
        return base_url.rstrip("/") + cleaned							# üè† –í—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö
    if not cleaned.startswith("http"):
        return base_url.rstrip("/") + "/" + cleaned.lstrip("/")		# üßµ –Ü–Ω—à–∏–π –≤—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö
    return cleaned														# ‚úÖ –í–∂–µ –∞–±—Å–æ–ª—é—Ç–Ω–∏–π


def _maybe_normalize(url_parser_service: UrlParserService, url: str) -> str:
    """üßº –í–∏–∫–ª–∏–∫–∞—î normalize, —è–∫—â–æ —Å–µ—Ä–≤—ñ—Å –π–æ–≥–æ –ø—ñ–¥—Ç—Ä–∏–º—É—î."""

    try:
        normalize = getattr(url_parser_service, "normalize", None)		# üß† –®—É–∫–∞—î–º–æ –º–µ—Ç–æ–¥
        if callable(normalize):
            result = normalize(url)										# type: ignore[no-any-return]
            return str(result or "").strip()							# üßΩ –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—á–∏—â–µ–Ω–∏–π URL
    except Exception:
        pass															# ü§´ –õ–æ–≥—ñ–∫—É normalize –Ω–µ –Ω–∞–≤'—è–∑—É—î–º–æ
    return url															# üîÅ –ë–µ–∑ –∑–º—ñ–Ω


def _get_attr_str(tag: Any, attr: str) -> str:
    """üè∑Ô∏è –î—ñ—Å—Ç–∞—î –∞—Ç—Ä–∏–±—É—Ç —Ç–µ–≥–∞ —è–∫ —Ä—è–¥–æ–∫."""

    if tag is None:
        return ""
    try:
        value = tag.get(attr)  # type: ignore[attr-defined]
    except Exception:
        return ""													# üö´ –ú–µ—Ç–æ–¥ get –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
    if isinstance(value, list):										# üß∫ BeautifulSoup –º–æ–∂–µ –ø–æ–≤–µ—Ä—Ç–∞—Ç–∏ list
        for item in value:
            if isinstance(item, str) and item:
                return item
        return ""
    return value if isinstance(value, str) else ""					# ‚úÖ –†—è–¥–æ–∫ –∞–±–æ –ø—É—Å—Ç–æ


def _get_script_text(script: Any) -> str:
    """üìú –ü–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞ (`string` –∞–±–æ `text`)."""

    if script is None:
        return ""
    try:
        direct = getattr(script, "string", None)						# üßµ string ‚Äî –±—ñ–ª—å—à —Ç–æ—á–Ω–∏–π
        if isinstance(direct, str) and direct:
            return direct
        fallback = getattr(script, "text", "")
        return fallback if isinstance(fallback, str) else ""
    except Exception:
        return ""


# ================================
# üèõÔ∏è –ü–ê–†–°–ï–† –ö–û–õ–ï–ö–¶–Ü–á
# ================================
class UniversalCollectionParser:
    """
    üèõÔ∏è –í–∏—Ç—è–≥—É—î –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ç–æ–≤–∞—Ä–∏ –∑ –∫–æ–ª–µ–∫—Ü—ñ–π YoungLA.

    –ö—Ä–æ–∫–∏:
      1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É —á–µ—Ä–µ–∑ `WebDriverService`.
      2. –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–∞–π—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏ –≤ JSON-LD.
      3. –Ø–∫—â–æ JSON-LD –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî fallback –Ω–∞ DOM —ñ–∑ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—î—é.
      4. –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ URL —Ç–∞ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Å–ø–∏—Å–æ–∫.
    """

    MIN_PAGE_LENGTH_BYTES = 1200											# üìè –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏

    PRODUCT_LINK_SELECTORS: Tuple[str, ...] = (							# üéØ –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤
        'a[href*="/products/"]',
        '[data-product-url*="/products/"]',
        '.product-card a[href*="/products/"]',
        '.grid-product a[href*="/products/"]',
        '.card a[href*="/products/"]',
        '.product-tile a[href*="/products/"]',
        '.product-item a[href*="/products/"]',
        '.collection-product-card a[href*="/products/"]',
        '.product-grid a[href*="/products/"]',
    )

    NEXT_SELECTORS: Tuple[str, ...] = (									# üîÅ –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó
        'link[rel="next"]',
        'a[rel="next"]',
        'a.pagination__next',
        'a[aria-label="Next"]',
        'a[title="Next"]',
        '.pagination a.next',
        '.pagination__item--next',
    )

    MAX_PAGINATION_PAGES = 5												# üö¶ –û–±–º–µ–∂–µ–Ω–Ω—è –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤

    def __init__(
        self,
        url: str,
        webdriver_service: WebDriverService,
        config_service: ConfigService,
        url_parser_service: UrlParserService,
        *,
        html_parser: str = "lxml",
    ) -> None:
        self.url = url													# üåê –ü–æ—Ç–æ—á–Ω–∏–π URL –∫–æ–ª–µ–∫—Ü—ñ—ó
        self.webdriver_service = webdriver_service						# üåç –°–µ—Ä–≤—ñ—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        self.config_service = config_service							# ‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥ INFRA
        self.url_parser_service = url_parser_service					# üåç –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è/–≤–∞–ª—é—Ç–∞
        self.html_parser = html_parser									# üßµ –û–±—Ä–∞–Ω–∏–π HTML-–ø–∞—Ä—Å–µ—Ä
        self.soup: Optional[BeautifulSoup] = None						# ü•£ Parsed DOM
        self.page_source: Optional[str] = None							# üßæ HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        self.currency: Optional[str] = self.url_parser_service.get_currency(self.url)  # üí± –ü–æ—Ç–æ—á–Ω–∞ –≤–∞–ª—é—Ç–∞

    # ================================
    # üîó –ü–£–ë–õ–Ü–ß–ù–ò–ô –ú–ï–¢–û–î
    # ================================
    async def get_product_links(self) -> List[str]:
        """üîó –ü–æ–≤–µ—Ä—Ç–∞—î —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Å–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ —Ç–æ–≤–∞—Ä–∏."""

        if not await self._fetch_page(self.url):							# üåê –°–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–µ—Ä—à—É —Å—Ç–æ—Ä—ñ–Ω–∫—É
            logger.warning("‚ùå –ö–æ–ª–µ–∫—Ü—ñ—è –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: %s", self.url)
            return []

        json_ld_links = self._parse_from_json_ld(self.soup)				# üìÑ –°–ø—Ä–æ–±–∞ —á–µ—Ä–µ–∑ JSON-LD
        if json_ld_links:
            logger.info("‚úÖ JSON-LD –¥–∞–≤ %d –ø–æ—Å–∏–ª–∞–Ω—å (–±–µ–∑ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó).", len(json_ld_links))
            return json_ld_links

        accumulated = self._parse_from_dom(self.soup)						# üåê DOM-fallback

        base_url = self._base_url()										# üè† –ë–∞–∑–æ–≤–∏–π –¥–æ–º–µ–Ω
        next_url = self._find_next_url(self.soup, base_url)				# üîÅ –ü–æ—à—É–∫ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        hops = 0															# üî¢ –õ—ñ—á–∏–ª—å–Ω–∏–∫ —Å—Ç–æ—Ä—ñ–Ω–æ–∫
        while next_url and hops < self.MAX_PAGINATION_PAGES:				# ‚è±Ô∏è –û–±–º–µ–∂—É—î–º–æ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é
            hops += 1
            if not await self._fetch_page(next_url):						# üåê –ü—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É
                break
            accumulated.extend(self._parse_from_dom(self.soup))			# ‚ûï –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –ª—ñ–Ω–∫–∏
            next_url = self._find_next_url(self.soup, base_url)			# üîÅ –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–∞–ª—ñ

        unique_links = _uniq_keep_order(accumulated)						# üîÅ –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ
        logger.info("üì¶ DOM-—Ä–µ–∂–∏–º: –∑—ñ–±—Ä–∞–Ω–æ %d –ø–æ—Å–∏–ª–∞–Ω—å (–≤–∫–ª—é—á–Ω–æ –∑ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—î—é).", len(unique_links))
        return unique_links

    # ================================
    # üïµÔ∏è‚Äç‚ôÇÔ∏è –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –°–¢–û–†–Ü–ù–ö–ò
    # ================================
    async def _fetch_page(self, url: str) -> bool:
        """üåê –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–∞ –≥–æ—Ç—É—î `BeautifulSoup`."""

        try:
            html = await self.webdriver_service.get_page_content(			# üåê –û—Ç—Ä–∏–º—É—î–º–æ HTML
                url,
                wait_until="networkidle",
                timeout_ms=30000,
                retries=1,
                retry_delay_sec=1,
                use_stealth=True,
            )
        except Exception as exc:
            logger.error("‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è %s: %s", url, exc)
            self.page_source = None										# üßπ –û—á–∏—â–∞—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            self.soup = None												# üßπ –û—á–∏—â–∞—î–º–æ –ø–∞—Ä—Å–µ—Ä
            return False

        self.page_source = html											# üßæ –ö–µ—à—É—î–º–æ —Å–∏—Ä–∏–π HTML
        if html and len(html) > self.MIN_PAGE_LENGTH_BYTES:				# üìè –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
            self.soup = BeautifulSoup(html, self.html_parser)				# ü•£ –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞—Ä—Å–µ—Ä
            logger.info("‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: %s", url)
            self.url = url												# üîÑ –û–Ω–æ–≤–ª—é—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π URL
            return True

        logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É –∞–±–æ –≤–æ–Ω–∞ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∞: %s", url)
        self.soup = None
        return False

    # ================================
    # üìÑ JSON-LD
    # ================================
    def _parse_from_json_ld(self, soup: Optional[BeautifulSoup]) -> List[str]:
        """üìÑ –í–∏—Ç—è–≥—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –±–ª–æ–∫—ñ–≤ JSON-LD."""

        if not soup:
            return []

        base_url = self._base_url()
        found: List[str] = []

        for script in soup.find_all("script", type="application/ld+json"):
            raw_obj = _safe_json_loads(_get_script_text(script))			# üßæ –†–æ–∑–ø–∞—Ä—Å–∏–º–æ JSON-LD
            if raw_obj is None:
                continue
            blocks = raw_obj if isinstance(raw_obj, list) else [raw_obj]	# üì¶ –£–Ω—ñ—Ñ—ñ–∫—É—î–º–æ —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫—ñ–≤
            for block in blocks:
                if not isinstance(block, dict):
                    continue

                atype = block.get("@type")
                types = {str(atype).lower()} if not isinstance(atype, list) else {str(item).lower() for item in atype}
                if not {"collectionpage", "searchresultspage", "itemlist"} & types:
                    continue												# ‚ùå –ë–ª–æ–∫ –Ω–µ —Å—Ö–æ–∂–∏–π –Ω–∞ –∫–æ–ª–µ–∫—Ü—ñ—é

                links = self._links_from_ld_collection(block)				# üîç –í–∏—Ç—è–≥—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ itemList
                if not links:
                    continue

                for raw_href in links:
                    abs_url = _ensure_abs(base_url, raw_href)				# üåê –î–æ–±—É–¥–æ–≤—É—î–º–æ –∞–±—Å–æ–ª—é—Ç–Ω–∏–π URL
                    abs_url = _strip_query_and_fragment(abs_url)			# üßº –û—á–∏—â–∞—î–º–æ –≤—ñ–¥ query/fragment
                    if _is_product_like_path(abs_url):
                        found.append(_maybe_normalize(self.url_parser_service, abs_url))

        return _uniq_keep_order(found)

    def _links_from_ld_collection(self, block: Dict[str, Any]) -> List[str]:
        """üì¶ –î–æ–ø–æ–º—ñ–∂–Ω–∏–π –º–µ—Ç–æ–¥: –≤–∏—Ç—è–≥—É—î href —ñ–∑ JSON-LD."""

        def _extract_href(node: Any) -> Optional[str]:
            if not node:
                return None
            if isinstance(node, str):
                return node												# üßæ –ü—Ä—è–º–∏–π URL
            if isinstance(node, dict):
                href = node.get("url") or node.get("@id") or node.get("identifier")
                if isinstance(href, str) and href:
                    return href											# üìé –ü–æ—á–∞—Ç–∫–æ–≤—ñ –ø–æ–ª—è –∑ URL
                node_type = node.get("@type")
                if isinstance(node_type, str) and node_type.lower() in {"listitem", "list_item"}:
                    return _extract_href(node.get("item"))				# üîÑ –†–µ–∫—É—Ä—Å—ñ—è –≤—Å–µ—Ä–µ–¥–∏–Ω—É item
                if "item" in node:
                    return _extract_href(node.get("item"))
            return None													# ‚ùå –ù—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏

        items: Any = None
        main_entity = block.get("mainEntity")
        if isinstance(main_entity, dict):
            items = main_entity.get("itemListElement")					# üîÑ –ß–∞—Å—Ç–æ mainEntity –º—ñ—Å—Ç–∏—Ç—å itemListElement
        if items is None:
            items = block.get("itemListElement")							# üîÅ –§–æ–ª–±–µ–∫ –Ω–∞ –≤–µ—Ä—Ö–Ω—ñ–π —Ä—ñ–≤–µ–Ω—å

        if items is None:
            return []
        if not isinstance(items, list):
            items = [items]												# üßæ –£–Ω—ñ—Ñ—ñ–∫—É—î–º–æ –¥–æ —Å–ø–∏—Å–∫—É

        out: List[str] = []
        for element in items:
            href = _extract_href(element)
            if isinstance(href, str) and href:
                out.append(href)
        return out

    # ================================
    # üåê DOM-FALLBACK
    # ================================
    def _parse_from_dom(self, soup: Optional[BeautifulSoup]) -> List[str]:
        """üåê –í–∏—Ç—è–≥—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ DOM, —è–∫—â–æ JSON-LD –ø–æ—Ä–æ–∂–Ω—ñ–π."""

        if not soup:
            return []

        base_url = self._base_url()
        acc: List[str] = []

        for selector in self.PRODUCT_LINK_SELECTORS:
            try:
                for element in soup.select(selector):
                    href = (_get_attr_str(element, "href") or _get_attr_str(element, "data-product-url")).strip()
                    if not href:
                        continue
                    href = _ensure_abs(base_url, href)					# üåê –ê–±—Å–æ–ª—é—Ç–Ω–∏–π URL
                    href = _strip_query_and_fragment(href)				# ‚úÇÔ∏è –û—á–∏—â–∞—î–º–æ
                    if _is_product_like_path(href):
                        acc.append(_maybe_normalize(self.url_parser_service, href))
            except Exception as exc:
                logger.warning("DOM selector failed '%s': %s", selector, exc)

        return _uniq_keep_order(acc)

    # ================================
    # üëâ –ü–ê–ì–Ü–ù–ê–¶–Ü–Ø
    # ================================
    def _find_next_url(self, soup: Optional[BeautifulSoup], base_url: str) -> Optional[str]:
        """üëâ –®—É–∫–∞—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É."""

        if not soup:
            return None

        tag = soup.select_one('link[rel="next"]')							# üîó –Ø–∫–Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π –≤–∏–ø–∞–¥–æ–∫
        href = _get_attr_str(tag, "href")
        if href:
            href = _ensure_abs(base_url, href)
            return _strip_query_and_fragment(href)

        for selector in self.NEXT_SELECTORS:								# üîÅ –ü—Ä–æ–±—É—î–º–æ –∫—ñ–ª—å–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤
            anchor = soup.select_one(selector)
            href = _get_attr_str(anchor, "href")
            if href:
                href = _ensure_abs(base_url, href)
                return _strip_query_and_fragment(href)

        try:
            pagination = soup.select_one(".pagination")
            if pagination:
                active = pagination.select_one(".active")
                if active:
                    next_anchor = active.find_next("a")
                    href = _get_attr_str(next_anchor, "href")
                    if href:
                        href = _ensure_abs(base_url, href)
                        return _strip_query_and_fragment(href)
        except Exception:
            pass															# ü§´ –ü–∞–≥—ñ–Ω–∞—Ü—ñ—è –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞

        return None

    # ================================
    # üîó –ë–ê–ó–ê –†–ï–ì–Ü–û–ù–£/–î–û–ú–ï–ù–£
    # ================================
    def _base_url(self) -> str:
        """üåç –ü–æ–≤–µ—Ä—Ç–∞—î –±–∞–∑–æ–≤–∏–π –¥–æ–º–µ–Ω –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –∞–±—Å–æ–ª—é—Ç–Ω–∏—Ö URL."""

        currency = (self.currency or "USD").upper()						# üí± –í–∞–ª—é—Ç–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        try:
            base = self.url_parser_service.get_base_url(currency)			# üåç –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤–∞–ª—é—Ç—É –≤ –±–∞–∑–æ–≤–∏–π –¥–æ–º–µ–Ω
            if isinstance(base, str) and base:
                return base
        except Exception as exc:
            logger.warning("url_parser_service.get_base_url failure: %s", exc)
        return "https://www.youngla.com"									# üè† –§–æ–ª–±–µ–∫: –≥–ª–æ–±–∞–ª—å–Ω–∏–π –¥–æ–º–µ–Ω
