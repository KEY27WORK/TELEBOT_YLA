""" ğŸ§¾ universal_collection_parser.py â€” Ğ£Ğ½Ñ–Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ ĞºĞ¾Ğ»ĞµĞºÑ†Ñ–Ğ¹ YoungLA (US, EU, UK).

ğŸ”¹ Ğ¤ÑƒĞ½ĞºÑ†Ñ–Ğ¾Ğ½Ğ°Ğ»:
- Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” Ñ€ĞµĞ³Ñ–Ğ¾Ğ½ ÑĞ°Ğ¹Ñ‚Ñƒ Ğ·Ğ° URL
- Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” HTML-ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ Ğ· WebDriverService
- ĞŸĞµÑ€ÑˆĞ¾Ñ‡ĞµÑ€Ğ³Ğ¾Ğ²Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ JSON-LD
- ĞœĞ°Ñ” fallback Ğ½Ğ° DOM-Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³
- Ğ’Ğ¸Ğ´Ğ°Ñ” ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² (href)

âœ… SOLID:
- SRP: Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ñ” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ·Ğ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ ĞºĞ¾Ğ»ĞµĞºÑ†Ñ–Ğ¹
- OCP: Ñ€Ğ¾Ğ·ÑˆĞ¸Ñ€ÑÑ”Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ğ·Ğ¼Ñ–Ğ½ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸ (Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ´Ğ¾Ğ´Ğ°Ñ‚Ğ¸ Ñ–Ğ½ÑˆÑ– Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸)
"""

import json
import logging
import asyncio
from bs4 import BeautifulSoup
from core.webdriver.webdriver_service import WebDriverService


class UniversalCollectionParser:
    """ ğŸ§¾ ĞŸĞ°Ñ€ÑĞµÑ€ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ğ¾Ğº ĞºĞ¾Ğ»ĞµĞºÑ†Ñ–Ğ¹ YoungLA Ğ· Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ĞºĞ¾Ñ Ñ€ĞµĞ³Ñ–Ğ¾Ğ½Ñ–Ğ² US ğŸ‡ºğŸ‡¸, EU ğŸ‡ªğŸ‡º, UK ğŸ‡¬ğŸ‡§.

    ĞÑĞ½Ğ¾Ğ²Ğ½Ñ– Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ—:
    - Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” Ğ²Ğ°Ğ»ÑÑ‚Ñƒ
    - Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” HTML-ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ
    - ĞŸĞµÑ€ÑˆĞ¾Ñ‡ĞµÑ€Ğ³Ğ¾Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ” Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ JSON-LD
    - ĞœĞ°Ñ” fallback Ğ½Ğ° DOM
    """

    def __init__(self, url: str):
        self.url = url
        self.soup = None
        self.page_source = None
        self.currency = self._detect_currency()

    def _detect_currency(self) -> str:
        """ ğŸŒ Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” Ğ²Ğ°Ğ»ÑÑ‚Ñƒ/Ñ€ĞµĞ³Ñ–Ğ¾Ğ½ Ğ·Ğ° URL.
        """
        if "eu." in self.url:
            return "EUR"
        elif "uk." in self.url:
            return "GBP"
        return "USD"

    async def fetch_page(self) -> bool:
        """
        ğŸŒ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” HTML ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ ĞºĞ¾Ğ»ĞµĞºÑ†Ñ–Ñ— Ñ‡ĞµÑ€ĞµĞ· WebDriver.
        """
        self.page_source = await asyncio.to_thread(WebDriverService().fetch_page_source, self.url)

        if self.page_source and len(self.page_source) > 1000:
            self.soup = BeautifulSoup(self.page_source, "html.parser")
            logging.info(f"âœ… Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ° ĞºĞ¾Ğ»ĞµĞºÑ†Ñ–Ñ— Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ°: {self.url}")
            return True

        logging.error(f"âŒ ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ: {self.url}")
        return False

    async def extract_product_links(self) -> list[str]:
        """ ğŸ”— Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ğ²ÑÑ– Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ Ğ½Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¸:
        - Ğ§ĞµÑ€ĞµĞ· JSON-LD
        - Ğ§ĞµÑ€ĞµĞ· DOM (fallback)

        :return: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº URL-Ğ°Ğ´Ñ€ĞµÑ
        """
        if not await self.fetch_page():
            logging.warning("âŒ Ğ¡Ñ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ° Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ° â€” Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº.")
            return []

        product_links = []

        # ğŸ” ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ JSON-LD
        for script in self.soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string.strip())
                if data.get("@type") == "CollectionPage" and "mainEntity" in data:
                    for item in data["mainEntity"].get("itemListElement", []):
                        url = item.get("item", {}).get("url")
                        if url:
                            product_links.append(url)

                if product_links:
                    logging.info(f"âœ… Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(product_links)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ñ‡ĞµÑ€ĞµĞ· JSON-LD")
                    return product_links

            except Exception as e:
                logging.warning(f"âš ï¸ JSON-LD Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³: {e}")

        # ğŸ” Ğ¯ĞºÑ‰Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ â€” Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼Ğ¾ DOM
        logging.info("ğŸ” JSON-LD Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹, Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ DOM...")

        try:
            product_elements = self.soup.select("a[href*='/products/']")
            for a in product_elements:
                href = a.get("href")
                if href and "/products/" in href:
                    full_url = self._build_full_url(href)
                    if full_url not in product_links:
                        product_links.append(full_url)

            if product_links:
                logging.info(f"ğŸ“¦ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(product_links)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ñ‡ĞµÑ€ĞµĞ· DOM.")
            else:
                logging.warning("âš ï¸ DOM-Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ½Ğµ Ğ´Ğ°Ğ² Ğ¶Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. ĞœĞ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾, Ğ·Ğ¼Ñ–Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ÑĞ°Ğ¹Ñ‚Ñƒ?")

        except Exception as e:
            logging.error(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ DOM: {e}")

        return product_links

    def _build_full_url(self, href: str) -> str:
        """ ğŸ—ï¸ Ğ¤Ğ¾Ñ€Ğ¼ÑƒÑ” Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ URL Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ğ²Ñ–Ğ´Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ.
        """
        base = "https://eu.youngla.com" if "eu." in self.url else \
               "https://uk.youngla.com" if "uk." in self.url else \
               "https://www.youngla.com"
        return href if href.startswith("http") else f"{base}{href}"

    def _get_domain(self) -> str:
        """ ğŸŒ ĞŸĞ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” Ğ´Ğ¾Ğ¼ĞµĞ½ ÑĞ°Ğ¹Ñ‚Ñƒ (Ğ±ĞµĞ· https://).
        """
        if "eu." in self.url:
            return "eu.youngla.com"
        elif "uk." in self.url:
            return "uk.youngla.com"
        return "www.youngla.com"

    def get_currency(self) -> str:
        """ ğŸ’± ĞŸĞ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” Ğ²Ğ°Ğ»ÑÑ‚Ñƒ ÑĞ°Ğ¹Ñ‚Ñƒ.
        """
        return self.currency